<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Composite//EN" "ditabase.dtd">
<topic id="topic1" xml:lang="en">
  <title id="iz173472">Using Resource Groups</title>
  <body>
    <note>Resource group-based workload management is a Greenplum Database Beta feature currently under development.</note>
    <p>You can use resource groups to manage the number of active queries that may execute concurrently in your Greenplum Database cluster. With resource groups, you can also manage the amount of CPU and memory resources Greenplum allocates to each query.</p>

    <p>When the user executes a query, Greenplum Database evaluates the query against a set of limits defined for the resource group. Greenplum executes the query immediately if the group's resource limits have not yet been reached and the query does not cause the group to exceed the concurrent transaction limit. If these conditions are not met, Greenplum Database queues the query. For example, if the maximum number of concurrent transactions for the resource group has already been reached, a subsequent query is queued and must wait until other queries complete before it runs.</p>
    <p>Within a resource group, transactions are evaluated on a first in, first out basis. Greenplum Database periodically assesses the active workload of the system, reallocating resources and starting/queuing jobs as necessary.</p>
    <p>When you create a resource group with the <codeph>CREATE RESOURCE GROUP</codeph> command, you provide a set of limits that together determine the amount of CPU and memory resources to allocate to transactions executed within the group. These limits are:</p>
     <table id="resgroup_limit_descriptions">
        <tgroup cols="3">
          <colspec colnum="1" colname="col1" colwidth="1*"/>
          <colspec colnum="2" colname="col2" colwidth="1*"/>
          <thead>
            <row>
              <entry colname="col1">Limit Type</entry>
              <entry colname="col2">Description</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry colname="col1">CONCURRENCY</entry>
              <entry colname="col2">The maximum number of concurrent transactions, including active and idle transactions, that are permitted for this resource group. </entry>
            </row>
            <row>
              <entry colname="col1">CPU_RATE_LIMIT</entry>
              <entry colname="col2">The percentage of CPU resources to allocate to this resource group.</entry>
            </row>
            <row>
              <entry colname="col1">MEMORY_LIMIT</entry>
              <entry colname="col2">The percentage of memory resources to allocate to this resource group.</entry>
            </row>
            <row>
              <entry colname="col1">MEMORY_SHARED_QUOTA</entry>
              <entry colname="col2">The percentage of memory to share across transactions submitted in this resource group.</entry>
            </row>
            <row>
              <entry colname="col1">MEMORY_SPILL_RATIO</entry>
              <entry colname="col2">The memory usage threshold for memory-intensive operators in a transaction. When the transaction reaches this threshold, it spills to disk.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

  </body>

  <topic id="topic8339717179" xml:lang="en">
    <title>Transaction Concurrency Limit</title>
    <body>
      <p>The <codeph>CONCURRENCY</codeph> limit controls the maximum number of concurrent transactions permitted for the resource group. Each resource group is logically divided into <codeph>CONCURRENCY</codeph> <varname>value</varname> number of slots. Greenplum Database allocates these slots an equal, fixed percentage of memory resources.</p>
      <p>The default <codeph>CONCURRENCY</codeph> limit value for a resource group is 20.</p>
      <p>Greenplum Database queues any transactions submitted after the resource group reaches its <codeph>CONCURRENCY</codeph> limit. When a running transaction completes, Greenplum Database un-queues and executes the earliest queued transaction if sufficient CPU and memory resources exist.</p>
    </body>
  </topic>

  <topic id="topic833971717" xml:lang="en">
    <title>CPU Limit</title>
    <body>
      <p>The <codeph>gp_resource_group_cpu_limit</codeph> server configuration parameter identifies the maximum percentage of system CPU resources assigned to the Greenplum Database node. The remaining CPU resources are used for the OS kernel and the Greenplum Database processes. The default <codeph>gp_resource_group_cpu_limit</codeph> value is .9 (90%).</p>
      <note>The default <codeph>gp_resource_group_cpu_limit</codeph> value may not leave sufficient CPU resources if you are running other workloads on your Greenplum Database cluster nodes, so be sure to adjust this server configuration parameter accordingly.</note>
      <p>The relative percentage of CPU resources available to Greenplum Database on a node is:
       <codeblock>greenplum_node_cpu_% = gp_resource_group_cpu_limit * 100</codeblock></p>
     <p>The Greenplum Database node CPU resource is further divided equally among each segment on the Greenplum node:
       <codeblock>segment_cpu_% = greenplum_node_cpu_%  / number_of_segments</codeblock></p>
     <p>Each resource group reserves a percentage of the <codeph>segment_cpu_%</codeph>. You identify this percentage via the <codeph>CPU_RATE_LIMIT</codeph> value you provide when you use the <codeph>CREATE RESOURCE GROUP</codeph> command to create the resource group:
        <codeblock>rg_cpu_% = segment_cpu_% * (CPU_RATE_LIMIT / 100)</codeblock></p>
      <p>The minimum <codeph>CPU_RATE_LIMIT</codeph> you can specify is for a resource group 1, the maximum is 100.</p>
      <p>The sum of <codeph>CPU_RATE_LIMIT</codeph>s specified for all resource groups you define in your Greenplum Database cluster must not exceed 100.</p>
      <p>CPU resource assignment is elastic in that Greenplum Database may allocate the CPU resources of an idle resource group to a busier one(s). In such situations, CPU resources are re-allocated to the previously idle resource group when that resource group next becomes active. If multiple resource groups are busy, they are allocated the CPU resources of any idle resource groups based on the ratio of their <codeph>CPU_RATE_LIMIT</codeph>s. For example, a resource group created with a <codeph>CPU_RATE_LIMIT</codeph> of 40 will be allocated twice as much extra CPU resource as a resource group you create with a <codeph>CPU_RATE_LIMIT</codeph> of 20.</p>
    </body>
  </topic>
  <topic id="topic8339717" xml:lang="en">
    <title>Memory Limits</title>
    <body>
      <p>When resource group-based workload management is enabled, memory usage is managed at the Greenplum Database node, segment, resource group, and transaction levels.</p>

      <p>The usable memory available on a Greenplum Database node is a function of the amount of RAM and swap space configured for the system, as well as the current <codeph>vm.overcommit_ratio</codeph> system parameter setting:
       <codeblock>total_node_memory = RAM * (vm.overcommit_ratio / 100) + SWAP</codeblock></p>

      <p>The <codeph>gp_resource_group_memory_limit</codeph> server configuration parameter identifies the maximum percentage of system memory resources assigned to the Greenplum Database node. The default <codeph>gp_resource_group_memory_limit</codeph> value is .9 (90%).
        <codeblock>greenplum_node_memory = total_node_memory * gp_resource_group_memory_limit</codeblock></p>
     <p>The memory resource available on a Greenplum Database node is further divided equally among each segment on the node:
       <codeblock>segment_memory = greenplum_node_memory / number_of_segments</codeblock></p>
     <note>Ensure that you set <codeph>gp_vmem_protect_limit</codeph> to a value  slightly larger than <codeph>segment_memory</codeph>.</note>
     <p>Each resource group reserves a percentage of the <codeph>segment_memory</codeph>. You identify this percentage via the <codeph>MEMORY_LIMIT</codeph> value you specify when you use the <codeph>CREATE RESOURCE GROUP</codeph> command to create the resource group:
        <codeblock>rg_memory = segment_memory * (MEMORY_LIMIT / 100)</codeblock></p>
      <p>The minimum <codeph>MEMORY_LIMIT</codeph> you can specify for a resource group is 1, the maximum is 100.</p>

      <p>The sum of <codeph>MEMORY_LIMIT</codeph>s specified for all resource groups you define in your Greenplum Database cluster must not exceed 100.</p>


      <p>The memory reserved by the resource group is divided into fixed and shared components. The <codeph>MEMORY_SHARED_QUOTA</codeph> value you specify when you create the resource group identifies the percentage of reserved resource group memory that may be shared among the currently running transactions. This memory is allotted on a first-come, first-served basis. 
        <codeblock>rg_mem_fixed = rg_memory * ((100 - MEMORY_SHARED_QUOTA) / 100)
rg_mem_shared = rg_memory * (MEMORY_SHARED_QUOTA / 100)</codeblock></p>
     <p>A running transaction may use none, some, or all of the <codeph>MEMORY_SHARED_QUOTA</codeph>.</p>

      <p>The minimum <codeph>MEMORY_SHARED_QUOTA</codeph> you can specify is 0, the maximum is 100. The default <codeph>MEMORY_SHARED_QUOTA</codeph> is 20.</p>

      <p>As mentioned previously, <codeph>CONCURRENCY</codeph> identifies the maximum number of concurrently running transactions permitted in the resource group. The memory reserved by a resource group is divided into <codeph>CONCURRENCY</codeph> number of transaction slots. Each slot is allocated a fixed, equal amount of resource group memory. Greenplum Database guarantees this fixed memory to each transaction.
        <codeblock>transaction_mem_fixed = rg_mem_fixed / CONCURRENCY</codeblock></p>
  
      <p>The maximum amount of resource group memory available to a specific transaction slot is the sum of the transaction's fixed memory and the full resource group shared memory quota:
<codeblock>transaction_mem_high = transaction_mem_fixed + rg_mem_shared</codeblock></p>

      <p><codeph>MEMORY_SPILL_RATIO</codeph> identifies the memory usage threshold for memory-intensive operators in a transaction. When the transaction reaches this memory threshold, it spills to disk.</p>
      <p>Greenplum Database uses the <codeph>MEMORY_SPILL_RATIO</codeph> to determine the initial memory to allocate to a transaction:
        <codeblock>transaction_initial_mem = (rg_mem_fixed * (MEMORY_SPILL_RATIO / 100)) / CONCURRENCY</codeblock></p>

      <p><codeph>transaction_initial_memory</codeph> is less than <codeph>transaction_mem_fixed</codeph> because transaction memory usage may keep increasing even after operator spill. When a query's memory usage exceeds the <codeph>transaction_mem_fixed</codeph> memory usage level, Greenplum Database allocates available <codeph>rg_mem_shared</codeph> to the query. The query will fail if and when there is insufficient <codeph>rg_mem_shared</codeph> to satisfy the query's memory requirements. </p>

      <p> The minimum <codeph>MEMORY_SPILL_RATIO</codeph> percentage you can specify for a resource group is 1. The maximum is 100. The upper limit of this ratio is (100 - <codeph>MEMORY_SHARED_QUOTA</codeph>). The default <codeph>MEMORY_SPILL_RATIO</codeph> is 20.</p>
      <p>The sum of <codeph>MEMORY_SHARED_QUOTA</codeph> and <codeph>MEMORY_SPILL_RATIO</codeph> must not exceed 100.</p>
 
    </body>
  </topic>

  <topic id="topic717179" xml:lang="en">
    <title>Resource Group Example</title>
    <body>
      <p>PLACEHOLDER</p>
    </body>
  </topic>

  <topic id="topic71717999" xml:lang="en">
    <title>Using Resource Group-Based Workload Management</title>
    <body>
      <p>To use resource groups in your Greenplum Database cluster, you must:</p>
      <ol>
        <li>Configure resource group-based workload management. See <xref href="#topic8339" format="dita"/>.</li>
        <li>Create resource groups. See <xref
            href="#topic10" type="topic" format="dita"/> and <xref href="#topic19" format="dita"/>.</li>
        <li>Assign the resource groups to one or more roles. See <xref href="#topic17" type="topic" format="dita"/>.</li>
        <li>Enable resource group-based workload management for your Greenplum Database cluster. See <xref href="#topic8" type="topic" format="dita"/>.</li>
        <li>Use workload management system views to monitor and manage the resource groups. See <xref href="#topic22" type="topic" format="dita"/>.</li>
      </ol>
    </body>
  </topic>

  <topic id="topic8339" xml:lang="en">
    <title>Configuring Greenplum Database for Resource Groups</title>
    <body>
      <p> Configuring your Greenplum Database cluster to support resource group-based workload management involves setting up Linux Control Groups and configuring server configuration parameters.</p>
    </body>

    <topic id="topic833" xml:lang="en">
      <title>Configuring cgroups for Greenplum Database</title>
      <body>
      <p>Greenplum Database resource group-based workload management uses Linux Control Groups (cgroups) to manage CPU resources. With cgroups, Greenplum isolates the CPU usage of your Greenplum processes from other processes on the node. This allows Greenplum to support CPU usage restrictions on a per-resource-group basis.</p>
      <p>For detailed information about cgroups, refer to the Control Groups documentation for your Linux distribution.</p>
        <p>Complete the following tasks on each node in your Greenplum Database cluster to set up cgroups for use with resource group-based workload management:</p>
        <ol>
          <li>Create the Greenplum Database cgroups configuration file <codeph>/etc/cgconfig.d/gpdb.conf</codeph>. You must be the superuser or have <codeph>sudo</codeph> access to create this file:
           <codeblock>sudo vi /etc/cgconfig.d/gpdb.conf</codeblock>
          </li>
          <li>Add the following configuration information to <codeph>/etc/cgconfig.d/gpdb.conf</codeph>:
           <codeblock>group gpdb {
     perm {
         task {
             uid = gpadmin;
             gid = gpadmin;
         }
         admin {
             uid = gpadmin;
             gid = gpadmin;
         }
     }
     cpu {
     }
     cpuacct {
     }
 } </codeblock> <p>This content configures CPU and CPU accounting control groups managed by the <codeph>gpadmin</codeph> user.</p>
        </li>
        <li>If not already installed and running, install the Control Groups operating system package and start the cgroups service on each Greenplum Database node. The commands you run to perform these tasks will differ based on the operating system installed on the node. You must be the superuser or have <codeph>sudo</codeph> access to run these commands:
        <ul>
          <li> Redhat/CentOS 7.x systems:
            <codeblock>sudo yum install libcgroup-tools
sudo cgconfigparser -l /etc/cgconfig.d/gpdb.conf </codeblock>
          </li>
          <li> Redhat/CentOS 6.x systems:
            <codeblock>sudo yum install libcgroup
sudo service cgconfig start </codeblock>
          </li>
          <li> Suse 11+ systems:
            <codeblock>sudo zypper install libcgroup-tools
sudo cgconfigparser -l /etc/cgconfig.d/gpdb.conf </codeblock>
          </li>
        </ul>
      </li>
          <li>Identify the <codeph>cgroup</codeph> directory mount point for the node:
        <codeblock>grep cgroup /proc/mounts</codeblock><p>The first line of output identifies the <codeph>cgroup</codeph> mount point.</p>
          </li>
          <li>Verify that you set up the Greenplum Database cgroups configuration correctly by running the following commands. Replace <varname>cgroup_mount_point</varname> with the mount point you identified in the previous step:
        <codeblock>ls -l <i>cgroup_mount_point</i>/cpu/gpdb
ls -l <i>cgroup_mount_point</i>/cpuacct/gpdb
        </codeblock> <p>If these directories exist and are owned by <codeph>gpadmin:gpadmin</codeph>, you have successfully configured cgroups for Greenplum Database CPU resource management.</p>
          </li>
        </ol>
      </body>
    </topic>
    <topic id="topic9" xml:lang="en">
     <title id="iz139711">Configuring Resource Group Parameters</title>
     <body>
      <p>Several Greenplum Database server configuration parameters affect the runtime behaviour of your cluster when you enable resource group-based workload management. </p>
      <p>The following table identifies resource group-related Greenplum Database server configuration parameters and describes their role in configuring resource group-based workload management:</p>
      <table id="resgroup_cfgparm_descriptions">
        <tgroup cols="3">
          <colspec colnum="1" colname="col1" colwidth="1*"/>
          <colspec colnum="2" colname="col2" colwidth="1*"/>
          <thead>
            <row>
              <entry colname="col1">Configuration Parameter</entry>
              <entry colname="col2">Description</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry colname="col1"><codeph>max_resource_groups</codeph></entry>
              <entry colname="col2">Sets the maximum number of resource groups</entry>
            </row>
            <row>
              <entry colname="col1"><codeph>gp_resource_group_cpu_limit</codeph></entry>
              <entry colname="col2">Identifies the percentage of system CPU resources to allot to Greenplum Database</entry>
            </row>
            <row>
              <entry colname="col1"><codeph>gp_resource_group_memory_limit</codeph></entry>
              <entry colname="col2">Identifies the percentage of system memory resources to allot to Greenplum Database</entry>
            </row>
          </tbody>
        </tgroup>
       </table>
     </body>
    </topic>
  </topic>

  <topic id="topic10" xml:lang="en">
    <title id="iz139857">Creating Resource Groups</title>
    <body>
      <p>When you create a resource group, you provide a name, CPU limit, and memory limit. You can optionally provide a concurrent transaction limit and memory shared quota and spill ratio.  Use the <codeph>CREATE RESOURCE GROUP</codeph> command to create a new resource group. </p>
      <p id="iz152723">When you create a resource group, you must provide <codeph>CPU_RATE_LIMIT</codeph> and <codeph>MEMORY_LIMIT</codeph> limit values.
          These limits identify the percentage of Greenplum Database resources
          to allocate to this resource group. For example, to create a resource group named
          <i>rgroup1</i> with a CPU limit of 20 and a memory limit of 25:</p>
        <p>
          <codeblock>=# CREATE RESOURCE GROUP <i>rgroup1</i> WITH (CPU_RATE_LIMIT=20, MEMORY_LIMIT=25);
</codeblock>
        </p>
        <p>The CPU limit of 20 is shared by every role to which <codeph>rgroup1</codeph> is assigned. Similarly, the memory limit of 25 is shared by every role to which <codeph>rgroup1</codeph> is assigned. <codeph>rgroup1</codeph> utilizes the default <codeph>CONCURRENCY</codeph> setting of 20.</p>
    </body>
  </topic>

  <topic id="topic17" xml:lang="en">
    <title id="iz172210">Assigning a Resource Group to a Role</title>
    <body>
      <p id="iz172211">When you create a resource group, the group is available for assignment to one or more roles (users). You assign a resource group to a database role using the <codeph>RESOURCE GROUP</codeph> clause of the <codeph>CREATE ROLE</codeph> or <codeph>ALTER ROLE</codeph> commands. If you do not specify a resource group for a role, the role is assigned the default group for the role's capability. <codeph>SUPERUSER</codeph> roles are assigned the <codeph>admin_group</codeph>, non-admin roles are assigned the group named <codeph>default_group</codeph>.</p>
      <p>The default resource groups <codeph>admin_group</codeph> and <codeph>default_group</codeph> have the following resource limits:</p>
       <table id="default_resgroup_limits">
        <tgroup cols="3">
          <colspec colnum="1" colname="col1" colwidth="1*"/>
          <colspec colnum="2" colname="col2" colwidth="1*"/>
          <colspec colnum="3" colname="col3" colwidth="1*"/>
          <thead>
            <row>
              <entry colname="col1">Limit Type</entry>
              <entry colname="col2">admin_group</entry>
              <entry colname="col3">default_group</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry colname="col1">CONCURRENCY</entry>
              <entry colname="col2">10</entry>
              <entry colname="col3">20</entry>
            </row>
            <row>
              <entry colname="col1">CPU_RATE_LIMIT</entry>
              <entry colname="col2">10</entry>
              <entry colname="col3">30</entry>
            </row>
            <row>
              <entry colname="col1">MEMORY_LIMIT</entry>
              <entry colname="col2">10</entry>
              <entry colname="col3">30</entry>
            </row>
            <row>
              <entry colname="col1">MEMORY_SHARED_QUOTA</entry>
              <entry colname="col2">50</entry>
              <entry colname="col3">50</entry>
            </row>
            <row>
              <entry colname="col1">MEMORY_SPILL_RATIO</entry>
              <entry colname="col2">20</entry>
              <entry colname="col3">20</entry>
            </row>
          </tbody>
        </tgroup>
       </table>
      <p>Use the <codeph>ALTER ROLE</codeph> or <codeph>CREATE ROLE</codeph> commands to assign a resource group to a role. For example:</p>
      <p>
        <codeblock>=# ALTER ROLE <i>bill</i> RESOURCE GROUP <i>rg_light</i>;
=# CREATE ROLE <i>mary</i> RESOURCE GROUP <i>exec</i>;
</codeblock>
      </p>
      <p>You can assign a resource group to one or more roles. If you have defined a role hierarchy, assigning a resource group to a parent role does not propagate down to the members of that role group.</p>
    </body>
  </topic>

    <topic id="topic18" xml:lang="en">
      <title id="iz153619">Removing a Role Assignment from a Resource Group</title>
      <body>
        <p>You may assign a resource group to one or more roles. If not explicitly assigned a particular group, the role is assigned the default group for the role's capability. If you wish to remove a resource group assignment from a role and assign the role the default group, change the role's group name assignment to <codeph>NONE</codeph>.
          For example:</p>
        <p>
          <codeblock>=# ALTER ROLE <i>mary</i> RESOURCE GROUP NONE;
</codeblock>
        </p>
      </body>
    </topic>

  <topic id="topic19" xml:lang="en">
    <title id="iz151541">Modifying Resource Groups</title>
    <body>
      <p>After you create a resource group, you may choose to change or reset the resource limits defined for the group with the <codeph>ALTER RESOURCE GROUP</codeph> command. You can remove a resource group with the <codeph>DROP RESOURCE GROUP</codeph> command. To change the resource group assigned to a specific role,
        see <xref href="#topic17" type="topic" format="dita"/>.</p>
    </body>
    <topic id="topic20" xml:lang="en">
      <title>Altering a Resource Group</title>
      <body>
        <p>The <codeph>ALTER RESOURCE GROUP</codeph> command updates the limits of a resource group.
          To change the limits of a resource group, specify the new values you want for the group.
          For example:</p>
        <p>
          <codeblock>=# ALTER RESOURCE GROUP <i>rg_light</i> SET CONCURRENCY 7;
=# ALTER RESOURCE GROUP <i>exec</i> SET MEMORY_LIMIT 25;
</codeblock>
        </p>
      </body>
    </topic>
    <topic id="topic21" xml:lang="en">
      <title>Dropping a Resource Group</title>
      <body>
        <p>The <codeph>DROP RESOURCE GROUP</codeph> command drops a resource group. To drop a resource group, the group cannot be assigned to any role, nor can there be any transactions active or waiting in the resource group.
          To drop a resource group:</p>
        <p>
          <codeblock>=# DROP RESOURCE GROUP <i>exec</i>;
</codeblock>
        </p>
      </body>
    </topic>
  </topic>

  <topic id="topic8" xml:lang="en">
    <title id="iz153124">Enabling Workload Management with Resource Groups</title>
    <body>
      <p>When you install Greenplum Database, workload management using resource queues is enabled by default. To enable resource group-based workload management in Greenplum Database, you must set the <codeph>gp_resource_manager</codeph> server configuration parameter.</p>
      <p>Ensure that you have configured cgroups as described in <xref href="#topic833" format="dita"/> before attempting to enable resource group-based workload management.</p>
      <ol id="ol_ec5_4dy_wq">
        <li>Set the <codeph>gp_resource_manager</codeph> server configuration parameter to the value <codeph>"group"</codeph>:
          <codeblock>gpconfig -s gp_resource_manager
gpconfig -c gp_resource_manager -v "group"
</codeblock>
        </li>
        <li>Restart Greenplum Database:
            <codeblock>gpstart
</codeblock>
        </li>
      </ol>
      <p>Once enabled, any transaction submitted by a role is directed to the resource group assigned to the role, and is governed by that resource group's concurrency, memory, and CPU limits.</p>
    </body>
  </topic>

  <topic id="topic22" xml:lang="en">
      <title id="iz152239">Monitoring Resource Group Status</title>
      <body>
        <p>Monitoring the status of your resource groups and queries may involve the following tasks:</p>
        <ul>
          <li id="iz153669"> <xref href="#topic221" type="topic" format="dita"/> </li>
          <li id="iz153670"> <xref href="#topic23" type="topic" format="dita"/> </li>
          <li id="iz153671"> <xref href="#topic25" type="topic" format="dita"/> </li>
          <li id="iz153679"> <xref href="#topic27" type="topic" format="dita"/> </li>
        </ul>
        
      </body>

    <topic id="topic221" xml:lang="en">
      <title id="iz152239">Viewing Resource Group Limits</title>
      <body>
        <p>The <codeph>gp_toolkit.resgroup_config</codeph> system view displays the current and proposed limits for a resource group. The proposed limit differs from the current limit when you alter the limit but the new value can not be immediately applied. To view the limits of all resource groups:</p>
        <p>
          <codeblock>=# SELECT * FROM gp_toolkit.gp_resgroup_config;
</codeblock>
        </p>
      </body>
    </topic>

    <topic id="topic23" xml:lang="en">
      <title id="iz152239">Viewing Resource Group Query Status and CPU/Memory Usage</title>
      <body>
        <p>The <codeph>gp_toolkit.resgroup_status</codeph> system view enables you to view the status and activity of a resource group. The view displays the number of running and queued transactions. It also displays the real-time CPU and memory usage of the resource group, averaged over all segments. To view this information:</p>
        <p>
          <codeblock>=# SELECT * FROM gp_toolkit.gp_resgroup_status;
</codeblock>
        </p>
      </body>
    </topic>

    <topic id="topic25" xml:lang="en">
      <title id="iz152239">Viewing the Resource Group Assigned to a Role</title>
      <body>
        <p>To view the resource group-to-role assignments, perform the following query on the
            <codeph>pg_roles</codeph> and
            <codeph>pg_resgroup</codeph> system catalog tables:</p>
        <p>
          <codeblock>=# SELECT rolname, rsgname FROM pg_roles, pg_resgroup
     WHERE pg_roles.rolresgroup=pg_resgroup.oid;
</codeblock>
        </p>
      </body>
    </topic>

    <topic id="topic27" xml:lang="en">
      <title id="iz153732">Canceling a Queued Transaction in a Resource Group</title>
      <body>
        <p>There may be cases when you want to cancel a queued transaction in a resource group. For
          example, you may want to remove a query that is waiting in the resource group queue but
          has not yet been executed. Or, you may want to stop a running query that is taking too
          long to execute, or one that is sitting idle in a transaction and taking up resource
          group transaction slots that are needed by other users.</p>
        <p>To cancel a queued transaction, you must first determine the process id (pid)
           associated with the transaction. Once you have obtained the  
           process id, you can invoke <codeph>pg_cancel_backend()</codeph> to end that process,
           as shown below.</p>
        <p>For example, to view the process information associated with all statements currently
          active or waiting in all resource groups, run the following query. If the query
          returns no results, then there are no running or queued transactions
          in any resource group.</p>
        <p>
          <codeblock>=# SELECT rolname, g.rsgname, procpid, waiting, current_query, datname 
     FROM pg_roles, gp_toolkit.gp_resgroup_status g, pg_stat_activity 
     WHERE pg_roles.rolresgroup=g.groupid
        AND pg_stat_activity.usename=pg_roles.rolname;
</codeblock>
        </p>

        <p>Sample partial query output:</p>
        <codeblock> rolname | rsgname  | procpid | waiting |     current_query     | datname 
---------+----------+---------+---------+-----------------------+---------
  sammy  | rg_light |  31861  |    f    | &lt;IDLE&gt; in transaction | testdb
  billy  | rg_light |  31905  |    t    | SELECT * FROM topten; | testdb</codeblock>
        <p>Use this output to identify the process id (<codeph>procpid</codeph>) of the transaction you want to cancel, and then cancel the process. For example, to cancel the pending query identified in the sample output above:</p>
        <p>
          <codeblock>=# pg_cancel_backend(31905) </codeblock>
        </p>
        <p>You can provide an optional message in a second argument to <codeph>pg_cancel_backend()</codeph>
          to indicate to the user why the process was cancelled.</p>
        <note type="note">
          <p>Do not use an operating system <codeph>KILL</codeph> command.</p>
        </note>
      </body>
    </topic>

  </topic>

</topic>
